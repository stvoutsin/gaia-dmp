#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


    Target:

        Test GaiaDMP with multiple users and record runtime

    Result:
  
        SUCCESS (But some slow)



     Notes:
       1) The tests were run using aglais-testing release: v0.1.5 (Note that this is not the version currently in main, but should be soon, as it includes changes for multi-user)
       2) The first tests were run using this notebook configuration: https://raw.githubusercontent.com/stvoutsin/aglais-testing/main/config/notebooks/notebooks-public-multi-user.json
           (Does not include dcr's notebooks)
          The second tests were run with the above & dcr's 2 notebooks
       3) Users were manually created in the shiro.ini file
       4) Tests were run from Python3 cli, using the aglais_benchmark tool that we installed.
       5) Tests were run using 1, 2, 4, and 8 users
       6) After each run, we removed the /tmp directory, and restarted Zeppelin. (This also stops any running Spark Jobs & Contexts)
 


# -----------------------------------------------
# Test 1:

# Benchmark config:

{
"notebooks" : [
           {
              "name" : "SetUp",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/SetUp.json",
              "totaltime" : 75,
              "results" : []
           },
           {
              "name" : "Mean_proper_motions_over_the_sky",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/Mean_proper_motions_over_the_sky.json",
              "totaltime" : 80,
              "results" : []
           },
           {
              "name" : "Source_counts_over_the_sky.json",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/Source_counts_over_the_sky.json",
              "totaltime" : 32,
              "results" : []
           },
           {
              "name" : "Good_astrometric_solutions_via_ML_Random_Forrest_classifier",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/Good_astrometric_solutions_via_ML_Random_Forrest_classifier.json",
              "totaltime" : 670,
              "results" : []
           } 

]
}





# -------------------------------------------------------

# Users: 1


# -----------------
# Run notebook
# root@ansibler

from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker("/deployments/zeppelin/test/config/notebooks-public-multi-user.json", "/tmp/").run(concurrent=False, users=1)


# Output

Test started [Single User]
Test completed! (662.74 seconds)
------------ Test Result: [SUCCESS] ------------

# Results
'SetUp': {'totaltime': '41.93', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '51.91', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '18.81', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '550.09', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}




# -------------------------------------------------------

# Users: 2

# -----------------
# Run notebook
# root@ansibler

from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker("/deployments/zeppelin/test/config/notebooks-public-multi-user.json", "/tmp/").run(concurrent=True, users=2)


# Output

Test started [Multi User]
Test completed! (928.06 seconds)
------------ Test Result: [SLOW] ------------


# Results:

# User1:

'SetUp': {'totaltime': '45.39', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '65.77', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '23.13', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'} 
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '793.74', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'} 

# User2:

'SetUp': {'totaltime': '52.05', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '60.62', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'} 
'Source_counts_over_the_sky.json': {'totaltime': '22.10', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '792.24', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}


# -------------------------------------------------------

# Users: 4


# -----------------
# Run notebook
# root@ansibler

from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker("/deployments/zeppelin/test/config/notebooks-public-multi-user.json", "/tmp/").run(concurrent=True, users=4)


# Output

Test started [Multi User]
Test completed! (2154.37 seconds)
------------ Test Result: [SLOW] ------------



# Results:


# User 1

'SetUp': {'totaltime': '44.61', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '72.69', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '27.09', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '786.82', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}


# User 2

'SetUp': {'totaltime': '54.64', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '64.61', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '26.53', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '785.46', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}


# User 3

'SetUp': {'totaltime': '931.15', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '56.96', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '16.90', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '547.97', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}


# User 4

'SetUp': {'totaltime': '931.19', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '472.89', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '127.75', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '622.52', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}




# -------------------------------------------------------

# Users: 8


# -----------------
# Run notebook
# root@ansibler

from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker("/deployments/zeppelin/test/config/notebooks-public-multi-user.json", "/tmp/").run(concurrent=True, users=8)


# Output

Test started [Multi User]
Test completed! (3272.05 seconds)
------------ Test Result: [SLOW] ------------


# Results:


# User 1

'SetUp': {'totaltime': '945.12', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '353.00', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '403.58', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '1257.11', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 2

'SetUp': {'totaltime': '946.17', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '736.98', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '264.86', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '764.09', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 3

'SetUp': {'totaltime': '945.05', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '125.36', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '25.43', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '815.86', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 4

'SetUp': {'totaltime': '941.60', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '121.26', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '32.77', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '970.62', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 5

'SetUp': {'totaltime': '69.48', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '58.27', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '26.22', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '812.54', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 6

'SetUp': {'totaltime': '54.02', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '59.98', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '20.02', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '738.84', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 7

'SetUp': {'totaltime': '945.03', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '214.59', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '433.15', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '1679.27', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}

# User 8

'SetUp': {'totaltime': '942.72', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '160.78', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '211.84', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '1844.38', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}}




# ----------------------------------------------------------------------------------------------------

# Test 2

# Test full notebooks, including dcr's notebooks

# Benchmark notebook config:


{
"notebooks" : [
           {
              "name" : "SetUp",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/SetUp.json",
              "totaltime" : 75,
              "results" : []
           },
           {
              "name" : "Mean_proper_motions_over_the_sky",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/Mean_proper_motions_over_the_sky.json",
              "totaltime" : 80,
              "results" : []
           },
           {
              "name" : "Source_counts_over_the_sky.json",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/Source_counts_over_the_sky.json",
              "totaltime" : 32,
              "results" : []
           },
           {
              "name" : "Good_astrometric_solutions_via_ML_Random_Forrest_classifier",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/Good_astrometric_solutions_via_ML_Random_Forrest_classifier.json",
              "totaltime" : 670,
              "results" : []
           },
           {
              "name" : "QC_cuts_dev.json",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/QC_cuts_dev.json",
              "totaltime" : 4700,
              "results" : []
           },
           {
              "name" : "WD_detection_dev.json",
              "filepath" : "https://raw.githubusercontent.com/wfau/aglais-testing/main/notebooks/public_examples/WD_detection_dev.json",
              "totaltime" : 3750,
              "results" : []
           }

]
}


# Users: 2

# -----------------
# Run notebook
# root@ansibler

from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker("/deployments/zeppelin/test/config/notebooks-public-multi-user.json", "/tmp/").run(concurrent=True, users=2)


# Output

Test started [Multi User]
Test completed! (16503.15 seconds)
------------ Test Result: [SLOW] ------------


# Results:


# User 1

'SetUp': {'totaltime': '52.40', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '67.70', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '27.63', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '796.93', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'QC_cuts_dev.json': {'totaltime': '7700.91', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'WD_detection_dev.json': {'totaltime': '7857.56', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}}, 


# User 2

'SetUp': {'totaltime': '44.33', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Mean_proper_motions_over_the_sky': {'totaltime': '73.93', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Source_counts_over_the_sky.json': {'totaltime': '25.22', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}
'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {'totaltime': '796.69', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'QC_cuts_dev.json': {'totaltime': '7490.39', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}
'WD_detection_dev.json': {'totaltime': '7522.43', 'status': 'SLOW', 'msg': '', 'valid': 'TRUE'}










# Summary / Notes on benchmarks:

It looks like in our current system, the first two jobs get about 45-50% of the resources, and each new job gets 1% of the cluster, but no executors, so any new jobs only start after the first two have completed.
This also shows in the result runtime. If we look at the runtime of the Setup script for the third user in our 4 user concurrent test, it is aproximately equal to the total runtime for either of the first user who's jobs completed.
This implies that it starts as soon as all of their (first user to complete ) notebooks have finished.
