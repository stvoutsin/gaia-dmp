#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


Target:
   Enable User impersonization for Zeppelin/Spark/Hadoop
Result:
   Success


# -------------------------------------------
# Set User Impersonate in Zeppelin

# In Zeppelin UI, Modify Spark interpreter to

The interpreter will be instantiated "Per User" in "isolated" process  
[y] User Impersonate 


# -----------------------------------------------------------------------------
# Modify Hadoop configuration (core-site.xml), allow proxy users through fedora
# Apply changes to all nodes (hadoop & zeppelin)

nano /opt/hadoop/etc/hadoop/core-site.xml
...

<property>
<name>hadoop.proxyuser.fedora.hosts</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.fedora.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.fedora.users</name>
<value>*</value>
</property>

...


# Restart hdfs..


# -----------------------------------------------------------------------------
# Create HDFS directory for new user, and set permission
# Otherwise we get a permission denied error as Spark tries to write to /user/ as the newuser but does not have permissions

hdfs dfs -mkdir /user/gaiauser
hdfs dfs -chown -R gaiauser:supergroup /user/gaiauser


# Run Spark job [SUCCESS]


# Repeat for another user



hdfs dfs -mkdir /user/gaiadmp
hdfs dfs -chown -R gaiadmp:supergroup /user/gaiadmp

# Run Spark job [SUCCESS]


# Check Hadoop UI

# Two jobs started, one as "gaiauser" and one as "gaiadmp"
# Both jobs started right away, and released resources after completion (except for 4% which they retain)

